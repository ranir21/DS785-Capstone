{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cee46c8a-3123-42f0-871e-db7ac0e24874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Delayed('int-9dc90954-f69c-4c34-addd-e8c7836c24ae'), 41)\n",
      "  Unique Key            Created Date             Closed Date Agency  \\\n",
      "0   56410339  12/31/2022 11:45:00 PM  01/03/2023 02:15:00 AM   DSNY   \n",
      "1   56410070  12/31/2022 11:45:00 PM  01/01/2023 12:45:00 AM    DOT   \n",
      "2   56413183  12/31/2022 11:44:50 PM  01/01/2023 12:30:23 AM   NYPD   \n",
      "3   56418399  12/31/2022 11:44:06 PM  01/01/2023 12:39:00 AM   NYPD   \n",
      "4   56414096  12/31/2022 11:43:56 PM  01/01/2023 06:18:59 AM    DHS   \n",
      "\n",
      "                       Agency Name              Complaint Type  \\\n",
      "0         Department of Sanitation           Derelict Vehicles   \n",
      "1     Department of Transportation    Traffic Signal Condition   \n",
      "2  New York City Police Department           Illegal Fireworks   \n",
      "3  New York City Police Department           Illegal Fireworks   \n",
      "4  Department of Homeless Services  Homeless Person Assistance   \n",
      "\n",
      "          Descriptor     Location Type Incident Zip       Incident Address  \\\n",
      "0  Derelict Vehicles            Street        11204         6010 16 AVENUE   \n",
      "1         Controller                          11368                          \n",
      "2                N/A   Street/Sidewalk        11385      1705 GATES AVENUE   \n",
      "3                N/A   Park/Playground        11223  1969 EAST    5 STREET   \n",
      "4                     Store/Commercial        10001   501 WEST   33 STREET   \n",
      "\n",
      "   ... Vehicle Type Taxi Company Borough Taxi Pick Up Location  \\\n",
      "0  ...                                                           \n",
      "1  ...                                                           \n",
      "2  ...                                                           \n",
      "3  ...                                                           \n",
      "4  ...                                                           \n",
      "\n",
      "  Bridge Highway Name Bridge Highway Direction Road Ramp  \\\n",
      "0                                                          \n",
      "1                                                          \n",
      "2                                                          \n",
      "3                                                          \n",
      "4                                                          \n",
      "\n",
      "  Bridge Highway Segment            Latitude           Longitude  \\\n",
      "0                         40.624731512699185    -73.992085560357   \n",
      "1                          40.74937520565083  -73.86751257995878   \n",
      "2                          40.70232942829639  -73.90938471087401   \n",
      "3                          40.60274696572387   -73.9677944791924   \n",
      "4                         40.754004936286634  -73.99981953120813   \n",
      "\n",
      "                                   Location  \n",
      "0    (40.624731512699185, -73.992085560357)  \n",
      "1   (40.74937520565083, -73.86751257995878)  \n",
      "2   (40.70232942829639, -73.90938471087401)  \n",
      "3    (40.60274696572387, -73.9677944791924)  \n",
      "4  (40.754004936286634, -73.99981953120813)  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# 1. Point to your CSV file (update the path as needed)\n",
    "path = \"D:\\\\Rekha\\\\Capstone\\\\Data\\\\311_Service_Requests_from_2010_to_Present_20250713.csv\"\n",
    "\n",
    "# 2. Read it in\n",
    "df = dd.read_csv(path, dtype=str, keep_default_na=False)\n",
    "\n",
    "# 3. Quick sanity‚Äêcheck\n",
    "print(df.shape)     # rows, columns\n",
    "print(df.head())    # first 5 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37396f91-96e3-490e-88e0-c335974c0bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-00.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-01.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-02.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-03.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-04.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-05.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-06.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-07.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-08.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-09.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-10.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-11.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-12.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-13.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-14.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-15.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-16.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-17.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-18.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-19.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-20.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-21.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-22.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-23.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-24.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-25.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-26.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-27.csv',\n",
       " 'D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-28.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example: drop rows missing key fields\n",
    "required = ['Unique Key','Incident Zip','Agency','Agency Name',\n",
    "            'Complaint Type','Status','Open Data Channel Type',\n",
    "            'Latitude','Longitude','Location','Created Date','Closed Date']\n",
    "df_clean = df.dropna(subset=required)\n",
    "\n",
    "df_clean.to_csv(\"D:\\\\Rekha\\\\Capstone\\\\Data\\\\cleaned\\\\cleaned-*.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "459d2134-f876-4888-8679-ca469c81107a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29 CSV files:\n",
      " - cleaned-00.csv\n",
      " - cleaned-01.csv\n",
      " - cleaned-02.csv\n",
      " - cleaned-03.csv\n",
      " - cleaned-04.csv\n",
      " - cleaned-05.csv\n",
      " - cleaned-06.csv\n",
      " - cleaned-07.csv\n",
      " - cleaned-08.csv\n",
      " - cleaned-09.csv\n",
      " - cleaned-10.csv\n",
      " - cleaned-11.csv\n",
      " - cleaned-12.csv\n",
      " - cleaned-13.csv\n",
      " - cleaned-14.csv\n",
      " - cleaned-15.csv\n",
      " - cleaned-16.csv\n",
      " - cleaned-17.csv\n",
      " - cleaned-18.csv\n",
      " - cleaned-19.csv\n",
      " - cleaned-20.csv\n",
      " - cleaned-21.csv\n",
      " - cleaned-22.csv\n",
      " - cleaned-23.csv\n",
      " - cleaned-24.csv\n",
      " - cleaned-25.csv\n",
      " - cleaned-26.csv\n",
      " - cleaned-27.csv\n",
      " - cleaned-28.csv\n",
      "Total rows after merge: 3169770\n",
      "Merged file written to: D:\\Rekha\\Capstone\\Data\\Cleaned\\311_merged.csv\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Define the directory and pattern\n",
    "input_dir = \"D:\\\\Rekha\\\\Capstone\\\\Data\\\\Cleaned\"\n",
    "pattern = os.path.join(input_dir, \"*.csv\")\n",
    "\n",
    "# 2. Locate all CSV files\n",
    "csv_files = sorted(glob.glob(pattern))\n",
    "print(f\"Found {len(csv_files)} CSV files:\")\n",
    "for f in csv_files:\n",
    "    print(\" -\", os.path.basename(f))\n",
    "\n",
    "# 3. Read and concatenate using pandas\n",
    "#    If memory is not an issue, pandas.concat is simplest\n",
    "dfs = [pd.read_csv(f, dtype=str) for f in csv_files]\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "print(f\"Total rows after merge: {len(merged_df)}\")\n",
    "\n",
    "# 4. Save merged CSV\n",
    "output_path = os.path.join(input_dir, \"311_merged.csv\")\n",
    "merged_df.to_csv(output_path, index=False)\n",
    "print(f\"Merged file written to: {output_path}\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a9bb61-1d83-468f-9aa8-15c4ac55978e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
